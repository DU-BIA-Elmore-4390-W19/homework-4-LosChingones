---
title: 'Homework 4: Bags, Forests, Boosts, oh my'
author: "Alejandro Arboleda"
date: "2/28/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r libs, message = F, warning = F, include = F}
library(tidyverse)
library(broom)
library(glmnet)
library(caret)
library(ISLR)
library(janitor)
library(stringr)
library(rpart)
library(rpart.plot)
library(partykit)
library(randomForest)
library(MASS)
library(gbm)
library(tree)
theme_set(theme_bw())
```
## Problem 1

Problem 7 from Chapter 8 in the text. To be specific, please use a sequence of
`ntree` from 25 to 500 in steps of 25 and `mtry` from 3 to 9 for by 1. 

## Answer 1
```{r}
set.seed(111)
df <- tbl_df(Boston)
inTraining <- createDataPartition(df$medv, p = .75, list = F)
training <- df[inTraining, ]
testing  <- df[-inTraining, ]
?sequence

for (t in seq (from = 25, to = 50, by = 25)){

rf_boston_cv <- train(medv ~ ., 
                      data = training,
                      method = "rf",
                      ntree = t,
                      importance = T,
                      tuneGrid = data.frame(mtry = 3:9))}

##put the RMSE into another dataframe

rf_boston_cv
```

##Compute the test MSE:

```{r}
test_preds <- predict(rf_boston_cv, newdata = testing)
boston_test_df <- testing %>%
  mutate(y_hat_bags = test_preds,
         sq_err_bags = (y_hat_bags - medv)^2)
mean(boston_test_df$sq_err_bags)
```
```{r}
plot(rf_boston_cv)
```

##Graph

```{r}
p <- ggplot(data = rf_boston_cv$results,
            aes(x = mtry, y = RMSE))
p + geom_point() +
  geom_line()
```

##Important Variables

```{r}
imp <- varImp(rf_boston_cv)$importance
rn <- row.names(imp)
imp_df <- data_frame(variable = rn, 
                     importance = imp$Overall) %>%
  arrange(desc(-importance)) %>%
  mutate(variable = factor(variable, variable))
p <- ggplot(data = imp_df,
            aes(variable, importance))
p + geom_col(fill = "#6e0000") +
  coord_flip()
```

## Problem 2

Problem 8 from Chapter 8 in the text. Set your seed with 9823 and split into 
train/test using 50\% of your data in each split. In addition to 
parts (a) - (e), do the following:

1. Fit a gradient-boosted tree to the training data and report the estimated 
test MSE. 
2. Fit a multiple regression model to the training data and report the 
estimated test MSE
3. Summarize your results. 

##a
```{r}
set.seed(9823)

train = sample(dim(Carseats)[1], dim(Carseats)[1]/2)
Carseats.train = Carseats[train, ]
Carseats.test = Carseats[-train, ]
```
##b
```{r}
tree.carseats = tree(Sales ~ ., data = Carseats.train)
summary(tree.carseats)
```

```{r}
plot(tree.carseats)
text(tree.carseats, pretty = 0)
```
##test MSE
```{r}
pred.carseats = predict(tree.carseats, Carseats.test)
mean((Carseats.test$Sales - pred.carseats)^2)
```
##c
```{r}
cv.carseats = cv.tree(tree.carseats, FUN = prune.tree)
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = "b")
plot(cv.carseats$k, cv.carseats$dev, type = "b")
```
##best size = 5
```{r}
pruned.carseats = prune.tree(tree.carseats, best = 5)
par(mfrow = c(1, 1))
plot(pruned.carseats)
text(pruned.carseats, pretty = 0)
```
##test MSE
```{r}
pred.pruned = predict(pruned.carseats, Carseats.test)
mean((Carseats.test$Sales - pred.pruned)^2)
```
##d
```{r}
bag.carseats = randomForest(Sales ~ ., data = Carseats.train, mtry = 10, ntree = 500, 
    importance = T)
bag.pred = predict(bag.carseats, Carseats.test)
mean((Carseats.test$Sales - bag.pred)^2)
```
##importance
```{r}
importance(bag.carseats)
```
##e
```{r}
rf.carseats = randomForest(Sales ~ ., data = Carseats.train, mtry = 25, ntree = 500, 
    importance = T)
rf.pred = predict(rf.carseats, Carseats.test)
mean((Carseats.test$Sales - rf.pred)^2)
```
##importance
```{r}
importance(rf.carseats)
```

